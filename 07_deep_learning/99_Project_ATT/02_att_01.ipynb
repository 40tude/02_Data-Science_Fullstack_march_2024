{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  AT&T Spam Detector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><b>TODO & Ideas - TO BE COMMENTED</b></span>\n",
    "\n",
    "* ~~In 03_bert.ipynb make a test with ``bert-base-uncased`` then ``bert-base-cased`` and compare~~\n",
    "* ~~Analyze results of the model~~\n",
    "* ~~Explain the number of trainable parameters of the model~~\n",
    "* ~~Use plotly to draw a confusion matrix per epoch. Can plotly do that ?~~\n",
    "* ~~Try over-under sampling (see 02_att_02.ipynb and 02_att_03.ipynb)~~\n",
    "* Make a test with an additional Dropout layer in the Baseline model to prevent overfitting?\n",
    "* See ``07_deep_learning\\08_Word_Embedding\\04-Embedding_for_sentiment_analysis.ipynb``\n",
    "* https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/\n",
    "* https://medium.com/@samia.khalid/bert-explained-a-complete-guide-with-theory-and-tutorial-3ac9ebc8fa7c\n",
    "* https://medium.com/@yashvardhanvs/classification-using-pre-trained-bert-model-transfer-learning-2d50f404ed4c\n",
    "* https://blog.mirkopeters.com/bert-based-transfer-learning-in-nlp-6887b4538bd0\n",
    "* https://thesai.org/Downloads/Volume12No1/Paper_64-Comparison_of_Deep_and_Traditional_Learning_Methods.pdf\n",
    "* https://medium.com/towards-data-science/spam-filtering-system-with-deep-learning-b8070b28f9e0\n",
    "* https://towardsdatascience.com/handling-overfitting-in-deep-learning-models-c760ee047c6e\n",
    "* https://www.tensorflow.org/tensorboard/get_started\n",
    "* https://www.geeksforgeeks.org/sms-spam-detection-using-tensorflow-in-python/\n",
    "* https://medium.com/@Coursesteach/deep-learning-part-37-bias-variance-3837b4f6caa1\n",
    "* https://www.kaggle.com/code/bishowlamsal/spam-detector-using-bert\n",
    "* https://www.kaggle.com/code/kshitij192/spam-email-classification-using-bert\n",
    "\n",
    "\n",
    "Since we will use TensorFlow for this project, I highly recommend to create a virtual environnement named ``tf_cpu1`` (python 3.10 is important for TF)\n",
    "\n",
    "```\n",
    "conda deactivate\n",
    "conda create --name tf_cpu1 --file ./assets/requirements.txt -c conda-forge -y\n",
    "conda activate tf_cpu1\n",
    "code .\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of the specs\n",
    "\n",
    "* https://app.jedha.co/course/projects-deep-learning-ft/att-spam-detector-ft\n",
    "\n",
    "## Goals\n",
    "* Your goal is to build a spam detector, that can automatically flag spams as they come based solely on the sms\" content.\n",
    "\n",
    "## Start simple\n",
    "* A good <span style=\"color:orange\"><b>deep learing model</b></span> does not necessarily have to be super complicated!\n",
    "\n",
    "## Transfer learning\n",
    "* You do not have access to a whole lot of data, perhaps channeling the power of a more sophisticated model trained on billions of observations might help!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prelude\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import en_core_web_sm\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from pathlib import Path\n",
    "# k_CurrentDir  = Path(__file__).parent    # __file__ is not known in Jupyter context\n",
    "k_Current_dir   = Path.cwd()\n",
    "k_AssetsDir     = \"assets\"\n",
    "k_Gold          = 1.618                    # gold number for ratio\n",
    "k_Width         = 12\n",
    "k_Height        = k_Width/k_Gold\n",
    "k_WidthPx       = 1024\n",
    "k_HeightPx      = k_WidthPx/k_Gold\n",
    "k_random_state  = 42\n",
    "k_test_size     = 0.3\n",
    "k_num_words     = 1_000                    # the number of most freq words to keep during tokenization\n",
    "k_epochs        = 50                       # I tried 10, 20, 50 and 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "def quick_View(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "    Generates a summary DataFrame for each column in the input DataFrame.\n",
    "\n",
    "    This function analyzes each column in the given DataFrame and creates a summary that includes\n",
    "    data type, number of null values, percentage of null values, number of non-null values, \n",
    "    number of distinct values, min and max values, outlier bounds (for numeric columns),\n",
    "    and the frequency of distinct values.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame to analyze.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the summary of each column from the input DataFrame. \n",
    "                      Each row in the resulting DataFrame represents a column from the input DataFrame\n",
    "                      with the following information:\n",
    "                      - \"name\": Column name\n",
    "                      - \"dtype\": Data type of the column\n",
    "                      - \"# null\": Number of null values\n",
    "                      - \"% null\": Percentage of null values\n",
    "                      - \"# NOT null\": Number of non-null values\n",
    "                      - \"distinct val\": Number of distinct values\n",
    "                      - \"-3*sig\": Lower bound for outliers (mean - 3*std) for numeric columns\n",
    "                      - \"min\": Minimum value for numeric columns\n",
    "                      - \"max\": Maximum value for numeric columns\n",
    "                      - \"+3*sig\": Upper bound for outliers (mean + 3*std) for numeric columns\n",
    "                      - \"distinct val count\": Dictionary of distinct value counts or top 10 values for object columns\n",
    "    \"\"\"\n",
    "\n",
    "    summary_lst = []\n",
    "  \n",
    "    for col_name in df.columns:\n",
    "        col_dtype               = df[col_name].dtype\n",
    "        num_of_null             = df[col_name].isnull().sum()\n",
    "        percent_of_null         = num_of_null/len(df)\n",
    "        num_of_non_null         = df[col_name].notnull().sum()\n",
    "        num_of_distinct_values  = df[col_name].nunique()\n",
    "\n",
    "        if num_of_distinct_values <= 10:\n",
    "            distinct_values_counts = df[col_name].value_counts().to_dict()\n",
    "        else:\n",
    "            top_10_values_counts    = df[col_name].value_counts().head(10).to_dict()\n",
    "            distinct_values_counts  = {k: v for k, v in sorted(top_10_values_counts.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "        if col_dtype != \"object\":\n",
    "            max_of_col = df[col_name].max()\n",
    "            min_of_col = df[col_name].min()\n",
    "            outlier_hi = df[col_name].mean() + 3*df[col_name].std()\n",
    "            outlier_lo = df[col_name].mean() - 3*df[col_name].std()\n",
    "        else:\n",
    "            max_of_col = -1\n",
    "            min_of_col =  1\n",
    "            outlier_hi = -1\n",
    "            outlier_lo =  1\n",
    "\n",
    "        summary_lst.append({\n",
    "            \"name\"                : col_name,\n",
    "            \"dtype\"               : col_dtype,\n",
    "            \"# null\"              : num_of_null,\n",
    "            \"% null\"              : (100*percent_of_null).round(2),\n",
    "            \"# NOT null\"          : num_of_non_null,\n",
    "            \"distinct val\"        : num_of_distinct_values,\n",
    "            \"-3*sig\"              : round(outlier_lo,2) ,\n",
    "            \"min\"                 : round(min_of_col,2),\n",
    "            \"max\"                 : round(max_of_col,2),\n",
    "            \"+3*sig\"              : round(outlier_hi,2) ,\n",
    "            \"distinct val count\"  : distinct_values_counts\n",
    "        })\n",
    "\n",
    "    df_tmp = pd.DataFrame(summary_lst)\n",
    "    return df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# drop empty cols and duplicates, rename cols...\n",
    "def cleaner(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    df.drop(columns=\"Unnamed: 2\", inplace=True)\n",
    "    df.drop(columns=\"Unnamed: 3\", inplace=True)\n",
    "    df.drop(columns=\"Unnamed: 4\", inplace=True)\n",
    "\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df.columns = df.columns.str.replace(\"/\", \"_\")\n",
    "\n",
    "    df.rename(columns={\"v1\": \"target\"}, inplace=True)\n",
    "    df.rename(columns={\"v2\": \"text\"}, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"https://full-stack-bigdata-datasets.s3.eu-west-3.amazonaws.com/Deep+Learning/project/spam.csv\", encoding=\"cp1252\")\n",
    "df = pd.read_csv(k_Current_dir / k_AssetsDir / \"spam.csv\", encoding=\"cp1252\")\n",
    "print(f\"\\n\\nPreview of the dataset (raw) :\")\n",
    "display(df.head())\n",
    "\n",
    "df = cleaner(df)\n",
    "# print(df.shape)\n",
    "\n",
    "print(f\"\\n\\nPreview of the initial dataset :\")\n",
    "display(df.head())\n",
    "\n",
    "print(f\"\\n\\nThe dataset consists of :\")\n",
    "print(f\"\\t{len(df.shape):>9_} dimensions\")\n",
    "print(f\"\\t{df.shape[0]:>9_} observations\")\n",
    "print(f\"\\t{df.shape[1]:>9_} features    \")\n",
    "\n",
    "print(f\"\\n\\nIt's a binary classification problem\")\n",
    "\n",
    "df_types = pd.DataFrame ({\n",
    "  \"types\" : df.dtypes.value_counts()\n",
    "})\n",
    "df_types[\"as_%\"] = (100 * df_types[\"types\"]/df_types[\"types\"].sum()).round(2)\n",
    "\n",
    "print(f\"\\n\\n% of data type :\")\n",
    "display(df_types)\n",
    "\n",
    "df_tmp = quick_View(df)\n",
    "print(f\"\\n\\nQuickView :\")\n",
    "display(df_tmp.sort_values(by=\"# null\", ascending=False))   \n",
    "\n",
    "print(f\"\\n\\n% of missing values :\")\n",
    "display(round(df.isnull().sum()/len(df)*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "* There is no missing values\n",
    "* 5k observations. Will it be enough ?\n",
    "* Unbalanced target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam & ham balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df[\"target\"].value_counts()\n",
    "print(f\"Nb spam : {counts['spam']:>7_}\")\n",
    "print(f\"Nb ham  : {counts['ham']:>7_}\")\n",
    "\n",
    "_ = counts.plot.pie(title=\"Weight as %\", autopct=\"%1.1f%%\", figsize=(k_Width, k_Height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "* No surprise, the target is heavily unbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How ham and spam text look like ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n\\nHAM : \")\n",
    "# pd.set_option(\"display.max_colwidth\", 1000)\n",
    "# print(df[df[\"target\"]==\"ham\"].head(20))\n",
    "print(df[df[\"target\"]==\"ham\"].head(20).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n\\nSPAM : \")\n",
    "print(df[df[\"target\"]==\"spam\"].head(20).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 2 lines below can help to print the punctuation signs\n",
    "# import string \n",
    "# string.punctuation\n",
    "\n",
    "# Remove punctuation  \n",
    "df[\"clean_docs\"] = df[\"text\"].apply(lambda x: re.sub(\"[!\\\"#$%&()*+,-./:;<=>?@\\[\\]^_`{|}~\\\\\\]+\",\" \", x)) \n",
    "\n",
    "# fillna() makes sure NA is replaced with \"\" so that lowering case do not generate error\n",
    "df[\"clean_docs\"] = df[\"clean_docs\"].fillna(\"\").apply(lambda x: x.lower())\n",
    "\n",
    "# df[\"clean_docs\"].head(20)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# Tokenize the cleaned document\n",
    "df[\"tokenized_docs\"] = df[\"clean_docs\"].fillna(\"\").apply(lambda x: nlp(x))\n",
    "\n",
    "# remove stop-words, replace words with their lemma\n",
    "df[\"tokenized_docs\"] = df[\"tokenized_docs\"].apply(lambda x: [token.lemma_ for token in x if token.text not in STOP_WORDS])\n",
    "# df[\"tokenized_docs\"].head(20)\n",
    "\n",
    "# clean up tokenized documents\n",
    "df[\"clean_tokens\"] = [\" \".join(x) for x in df[\"tokenized_docs\"]]\n",
    "\n",
    "# set the target as boolean value (spam=1) \n",
    "df[\"target\"] = df[\"target\"].map({\"ham\":0,\"spam\":1})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in set(df[\"target\"]):\n",
    "    words = \"\"\n",
    "    for doc in df[df[\"target\"] == i][\"text\"]:\n",
    "        words += doc + \" \"\n",
    "    wordcloud = WordCloud().generate(words)\n",
    "    plt.figure(figsize = (k_Height, k_Width))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"WordCloud for {'spam' if i==1 else 'ham'}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "\n",
    "* With no surprise \"free\", \"call\", \"now\"... are dominant in spams sms\n",
    "* Surprisingly \"sex\" and \"viagra\" are missing 😁"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_occurences_report(target):\n",
    "    df_tmp = \" \".join(df[df[\"target\"] == target][\"clean_tokens\"])\n",
    "    df_tmp = pd.DataFrame(df_tmp.split(\" \"))\n",
    "    df_tmp = df_tmp.value_counts(ascending=False)\n",
    "\n",
    "    df_tmp = df_tmp.reset_index()  \n",
    "    df_tmp.columns = [\"word\", \"occurrences\"]  \n",
    "    df_tmp.set_index(\"word\", inplace=True)  \n",
    "    display(df_tmp)\n",
    "\n",
    "    # print(f\"About words in ham :\")\n",
    "    print(f\"{len(df_tmp):>6_} differents words\")\n",
    "    print(f\"{df_tmp['occurrences'].sum():>6_} occurrences\")\n",
    "    pareto = int(df_tmp['occurrences'].sum()*0.8)\n",
    "    print(f\"{pareto:>6_} = 80% of thoses occurrences\")\n",
    "\n",
    "    i = 100\n",
    "    while df_tmp['occurrences'].head(i).sum()<pareto:\n",
    "        i+=100\n",
    "\n",
    "    print(f\"{i:6_} ({100*i/len(df_tmp):.0f} %) words are needed to cover 80% of the occurrences\")\n",
    "    print(f\"\\n\\n\\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"About words in spam :\")\n",
    "word_occurences_report(1)\n",
    "\n",
    "print(f\"About words in ham :\")\n",
    "word_occurences_report(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oov_token= out of vocabulary token\n",
    "# When the oov_token is specified in the tokenizer, words not present in the learned vocabulary will be replaced by this token. \n",
    "# This enables the model to handle new words that appear in test or inference data, while reducing the risk of errors or inaccuracies.\n",
    "# In a new sentence, 2 unknown words will be represented by the OOV token, preserving information even if the model hasn't seen these words before\n",
    "\n",
    "# If oov_token not specified any word not in the vocabulary will be ignored and not tokenized\n",
    "# This may result in the loss of important information during inference or testing.\n",
    "\n",
    "# keep the 1_000 most frequents words during tokenization\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=k_num_words, oov_token=\"_UNKNOWN_\") \n",
    "tokenizer.fit_on_texts(df[\"clean_tokens\"])\n",
    "df[\"sms_encoded\"] = tokenizer.texts_to_sequences(df[\"clean_tokens\"])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow cannot create a tensor dataset based on lists\n",
    "# We have to store encoded sms into a numpy array before creating the tensorflow dataset\n",
    "# However, not all the sequences are the same length\n",
    "# This is where `tf.keras.preprocessing.sequence.pad_sequences()` comes in \n",
    "# It will add zero padding at the beginning (`padding=\"pre\"`) or at the end (`padding=\"post\"`) of our sequences so they all have equal length\n",
    "sms_padded = tf.keras.preprocessing.sequence.pad_sequences(df[\"sms_encoded\"], padding=\"post\")\n",
    "print(sms_padded)\n",
    "print(sms_padded.shape)\n",
    "\n",
    "# max_words = df['clean_tokens'].apply(lambda x: len(x.split())).max()\n",
    "# print(max_words)\n",
    "# 77"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratify\n",
    "X_train, X_test, y_train, y_test = train_test_split(sms_padded, df[\"target\"], test_size = k_test_size, random_state = k_random_state, stratify = df[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "\n",
    "* No over/under sampling on ``X_train``, ``y_train``\n",
    "* See ``02_att_02.ipynb`` or ``02_att_03.ipynb``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a TensorFlow Dataset object to be used for data loading, batching, shuffling, and preprocessing when training the model\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generators, making sure that data are blended/mixed and divided into batches of 64 observations \n",
    "train_batch = train_ds.shuffle(len(train_ds)).batch(64)\n",
    "test_batch = test_ds.shuffle(len(test_ds)).batch(64)\n",
    "\n",
    "# for sms, ham_spam in train_batch.take(1):\n",
    "#   print(sms, ham_spam)\n",
    "\n",
    "# print()\n",
    "# print(sms.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Baseline Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = sms_padded.shape[1]\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # This Embedding layer converts integer-encoded words (from the tokenizer) into dense vectors of size 8 \n",
    "    # tokenizer.num_words should be equal to k_num_words\n",
    "    # +1 because TensorFlow reserves an index for padding or the OOV token\n",
    "    # sms.shape[1] is the length of the input sequence. See the output of the previous cell (77)\n",
    "    # For the output I tried : 16 8 4 and 2\n",
    "    # I like 8\n",
    "    \n",
    "    # tf.keras.layers.Embedding(tokenizer.num_words + 1, 8, input_shape=[sms.shape[1],], name=\"embedding\"),\n",
    "    tf.keras.layers.Embedding(tokenizer.num_words + 1, 8, input_shape=[sequence_length], name=\"embedding\"),\n",
    "\n",
    "    # Global average pooling\n",
    "    # Reduces the dimensionality by averaging the vectors across the sequence length\n",
    "    # The model loose the order of the words but we don't care  \n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "\n",
    "    # Fully connected (Dense) layer with 16 neurons. \n",
    "    # relu activation function introduces non-linearity (this helps the model to capture more complex patterns)\n",
    "    tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "\n",
    "    # Since this is a binary classification problem (based on the sigmoid activation), there's a single output neuron. \n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(f\"{k_Current_dir/k_AssetsDir/'baseline_arch.png'}\")\n",
    "tf.keras.utils.plot_model(model, path, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "\n",
    "About the number of trainable parameters of the model\n",
    "\n",
    "```\n",
    "Inputs                 : 1000 words    (see k_num_words in prelude)\n",
    "Embedding layer        :  8 neurons => (1000 + 1) * 8 = 8008 params\n",
    "GlobalAveragePooling1D :    average => no params      =    0 params\n",
    "Dense layer            : 16 neurons => 8  * 16 + 16   =  144 params\n",
    "Dense layer            :  1 neuron  => 16 *  1 +  1   =   17 params\n",
    "                                                TOTAL = 8169 params\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.Recall(name=\"recall\"), tf.keras.metrics.Precision(name=\"precision\"), \"accuracy\"]       # name=... avoid recall_1 for example\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrices = []\n",
    "\n",
    "def get_data_from_dataset(dataset):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for batch_features, batch_labels in dataset:\n",
    "        features.append(batch_features.numpy())\n",
    "        labels.append(batch_labels.numpy())\n",
    "    return np.concatenate(features), np.concatenate(labels)\n",
    "\n",
    "class ConfusionMatrixCallback(tf.keras.callbacks.Callback):             # inherit from tf.keras.callbacks.Callback\n",
    "    def __init__(self, val_data):\n",
    "        # store data once\n",
    "        self.val_data = val_data                                        \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # extract features and labels from validation set\n",
    "        val_features, val_labels = get_data_from_dataset(self.val_data)\n",
    "        \n",
    "        # make prediction with validation features\n",
    "        val_pred = (self.model.predict(val_features) > 0.5).astype(\"int32\")\n",
    "        \n",
    "        # compute the associated confusion matrix\n",
    "        cm = confusion_matrix(val_labels, val_pred)\n",
    "\n",
    "        # flip the 2 lines just to make sure it looks like sklearn.metrics.confusion_matrix\n",
    "        cm_flipped = np.flipud(cm)\n",
    "        \n",
    "        confusion_matrices.append(cm_flipped)\n",
    "\n",
    "history = model.fit(\n",
    "    train_batch,\n",
    "    epochs=k_epochs,\n",
    "    validation_data=test_batch,\n",
    "    callbacks=[ConfusionMatrixCallback(test_batch)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\"><b>Sync point :</b></span>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./assets/metrics.png\" alt=\"drawing\" width=\"400\"/>\n",
    "<p>\n",
    "\n",
    "* Sms identified as spam (=1)\n",
    "* Can I accept spam in my sms? I don't know...\n",
    "* Can I accept to see an sms from my beloved CEO classified as spam? No! \n",
    "* So I want FP to tend towards 0 and precision $\\frac{TP}{TP+FP}$ towards 1 (even if the recall, $\\frac{TP}{TP+FN}$, is not that great )\n",
    "* I decide to favour precision over recall\n",
    "\n",
    "Just to make sure...\n",
    "* **Overfitting**\n",
    "    * If accuracy continues to increase on training but starts to decrease or stagnate on validation\n",
    "    * this indicates overfitting.\n",
    "\n",
    "* **Underfitting** \n",
    "    * If the loss curves do not decrease \n",
    "    * or \n",
    "    * If accuracy remains low\n",
    "    * this could indicate that the model has not yet learned sufficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = Path(f\"{k_Current_dir/k_AssetsDir/fig_id}.{fig_extension}\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"val_\" stands for validation\n",
    "# history.history.keys()\n",
    "\n",
    "plt.plot(history.history[\"loss\"], color=\"b\", label=\"Train Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], color=\"r\", label=\"Val Loss\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Baseline : Loss\")\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "save_fig(\"baseline_loss\", \"png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# display(history.history['loss'][-10:])\n",
    "display([[round(f, 6) for f in history.history['loss'][-10:]]])\n",
    "display([[round(f, 6) for f in history.history['val_loss'][-10:]]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "* Interpretation\n",
    "    * The loss measures the error of the model on the training data\n",
    "    * The learning rate seems OK because the curve shows a drop and then a plateau\n",
    "    * The 2 curves don't separate immediately. The model doesn't learn \"by heart\". This is a good indicator\n",
    "    * Validation loss rises while train loss remains on its plateau. This is also a good sign\n",
    "* Analysis\n",
    "    * A loss of 0.05 is relatively low, indicating that the model has learned the characteristics of the data pretty well \n",
    "    * However, a low loss does not necessarily mean that the model is perfect \n",
    "    * See comments about other metrics below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"accuracy\"], color=\"b\", label=\"Train Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], color=\"r\", label=\"Val Accuracy\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Baseline : Accuracy\")\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "save_fig(\"baseline_accuracy\", \"png\")\n",
    "plt.show()\n",
    "\n",
    "display([[round(f, 6) for f in history.history['accuracy'][-10:]]])\n",
    "display([[round(f, 6) for f in history.history['val_accuracy'][-10:]]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "* Interpretation\n",
    "    * Accuracy measures the total proportion of correct predictions (TP + TN) out of all predictions made (all cells of the matrix above)\n",
    "    * Diagonal of the matrix (see above, synch point)\n",
    "    * An accuracy of 0.985 means that the model correctly classifies 98.5% of emails\n",
    "* Analysis: \n",
    "    * High accuracy is generally a good sign\n",
    "    * **BUT** in a spam detection problem where the classes are unbalanced (much more non-spam than spam), high accuracy can be misleading \n",
    "    * For example, predicting that all emails are non-spam could give a good accuracy if spam is rare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"precision\"], color=\"b\", label=\"Train Precision\")\n",
    "plt.plot(history.history[\"val_precision\"], color=\"r\", label=\"Val Precision\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Baseline : Precision\")\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "save_fig(\"baseline_precision\", \"png\")\n",
    "plt.show()\n",
    "\n",
    "display([[round(f, 6) for f in history.history['precision'][-10:]]])\n",
    "display([[round(f, 6) for f in history.history['val_precision'][-10:]]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "* Interpretation\n",
    "    * Precision measures the proportion of true positives (sms correctly identified as spam) versus all sms classified as spam by the model (TP + FP). \n",
    "    * Right hand side column of the matrix above\n",
    "    * A precision of 0.994 means that when the model predicts that an sms is spam, it is right 99.4% of the time.\n",
    "* Analysis\n",
    "    * Precision is excellent and this is what we want\n",
    "    * The model is very effective at minimizing false positives, i.e. non-spam sms wrongly classified as spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"recall\"], color=\"b\", label=\"Train Recall\")\n",
    "plt.plot(history.history[\"val_recall\"], color=\"r\", label=\"Val Recall\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Baseline : Recall\")\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "save_fig(\"baseline_recall\", \"png\")\n",
    "plt.show()\n",
    "\n",
    "display([[round(f, 6) for f in history.history['recall'][-10:]]])\n",
    "display([[round(f, 6) for f in history.history['val_recall'][-10:]]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "\n",
    "\n",
    "* Interpretation\n",
    "    * Recall (AKA sensitivity) measures the proportion of true positives (correctly identified spam sms) to all true spam (TP + FN)\n",
    "    * Bottom line of the matrix above\n",
    "    * A recall of 0.892 means that the model correctly detects 89.2% of sms that are really spam\n",
    "* Analysis\n",
    "    * Although the recall is relatively high, it is still missing around 10.8% of spam (false negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_calculus(name, rec, prec):\n",
    "    df_tmp=pd.DataFrame()\n",
    "    df_tmp[name] = 2*np.array(rec)*np.array(prec)/(np.array(rec)+np.array(prec)+tf.keras.backend.epsilon()) # epsilon avoid runtimeWarning: divide by zero encountered in divide...\n",
    "    return df_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = f1_calculus(\"f1\", history.history[\"recall\"], history.history[\"precision\"])\n",
    "df_val_tmp = f1_calculus(\"val_f1\", history.history[\"val_recall\"], history.history[\"val_precision\"])\n",
    "\n",
    "plt.plot(df_tmp[\"f1\"], color=\"b\", label=\"Train F1\")\n",
    "plt.plot(df_val_tmp[\"val_f1\"], color=\"r\", label=\"Val F1\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Baseline : F1\")\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "save_fig(\"baseline_f1\", \"png\")\n",
    "plt.show()\n",
    "\n",
    "display(df_tmp.tail(10))\n",
    "display(df_val_tmp.tail(10))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "* Interpretation: \n",
    "    * F1 score provides a balanced measure of the model when there's a trade-off between precision and recall \n",
    "    * It is the harmonic mean between precision and recall\n",
    "    * An F1 score of 0.94 shows that the model has a good balance between precision and recall\n",
    "* Analysis: \n",
    "    * The F1 score confirms that the model handles the trade-off between precision and recall pretty well\n",
    "    * This is crucial in spam detection problems where false negatives (undetected spam sms) can be annoying. \n",
    "        * It is great to NOT have true sms in spam but it could be great to have no spam at all in our sms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix (animated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(confusion_matrices):\n",
    "    epochs = range(1, k_epochs + 1)\n",
    "    frames = []\n",
    "\n",
    "    # Create a frame per epoch\n",
    "    for epoch, cm in zip(epochs, confusion_matrices):\n",
    "        annotations = [[f'{value}' for value in row] for row in cm]\n",
    "        frame = go.Frame(data=[go.Heatmap(\n",
    "            z=cm, \n",
    "            text=annotations,\n",
    "            colorscale='Viridis',                       # mimic default color of sklearn.metrics.confusion_matrix()\n",
    "            hoverinfo='skip',                           # ! Hide hover info (x, y, z)\n",
    "            showscale=True,                             # colorbar visible\n",
    "            texttemplate=\"%{text}\",                     # string to be displayed in the cells\n",
    "            textfont={\"size\": 14},  \n",
    "        )], name=f\"Epoch {epoch}\")\n",
    "        frames.append(frame)\n",
    "\n",
    "    # Initialization with the first confusion matrix\n",
    "    fig = go.Figure(\n",
    "        data=[go.Heatmap(\n",
    "            z=confusion_matrices[0], \n",
    "            text=[[f'{value}' for value in row] for row in confusion_matrices[0]], \n",
    "            colorscale='Viridis',\n",
    "            hoverinfo='skip',  \n",
    "            showscale=True,  \n",
    "            texttemplate=\"%{text}\",  \n",
    "            textfont={\"size\": 14},  \n",
    "        )],\n",
    "        layout=go.Layout(\n",
    "            autosize=False,\n",
    "            width=1000,\n",
    "            height=1000,\n",
    "            title=\"Animated confusion matrix\",\n",
    "            xaxis_title=\"Predicted label\",\n",
    "            yaxis_title=\"True Label\",\n",
    "            # Make sure 0 is on top and 1 is bottom on y axis \n",
    "            # Make sure we read \"Ham\" and \"Spam\"\n",
    "            xaxis=dict(\n",
    "                tickvals=[0, 1], \n",
    "                ticktext=[\"Ham\", \"Spam\"]\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                tickvals=[0, 1],  \n",
    "                ticktext=[\"Spam\", \"Ham\"],  \n",
    "                categoryorder=\"array\",  \n",
    "                categoryarray=[1, 0]  \n",
    "            ),\n",
    "            updatemenus=[{\n",
    "                'type': 'buttons',\n",
    "                'buttons': [{\n",
    "                    'label': 'Play',\n",
    "                    'method': 'animate',\n",
    "                    'args': [None, {\n",
    "                        'frame': {'duration': 500, 'redraw': True},\n",
    "                        'fromcurrent': True,\n",
    "                        'mode': 'immediate',\n",
    "                    }]\n",
    "                }]\n",
    "            }]\n",
    "        ),\n",
    "        frames=frames  # assign frames to the Figure\n",
    "    )\n",
    "\n",
    "    # Add a slider\n",
    "    fig.update_layout(\n",
    "        sliders=[{\n",
    "            'steps': [{\n",
    "                'args': [[f\"Epoch {epoch}\"], {'frame': {'duration': 500, 'redraw': True}}],\n",
    "                'label': f'Epoch {epoch}',\n",
    "                'method': 'animate'\n",
    "            } for epoch in epochs],\n",
    "            'currentvalue': {'prefix': 'Epoch: '}\n",
    "        }]\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "plot_confusion_matrices(confusion_matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\"><b>Conclusion (baseline model evaluation) :</b></span>\n",
    "* Overall, the baseline model performs well and have good metrics\n",
    "* However, there are a few areas for improvement\n",
    "    * Recall (89% => 11%) \n",
    "    * With the others model, let's try to improve the recall without loosing to much on the precision\n",
    "    * Is it possible with such small dataset ?\n",
    "    * Should we consider techniques to balance the classes (SMOTE...)?\n",
    "    * Should we put the model in production and collect feedback and see how it goes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using RNN with LSTM\n",
    "\n",
    "* Recurrent Neural Networks\n",
    "* Long Short-Term Memory\n",
    "* Advantages:\n",
    "    * Efficiently capture word relationships\n",
    "    * Handles long, complex sequences well\n",
    "* Disadvantages:\n",
    "    * Much slower to train than CNN-type models (see below)\n",
    "    * Risk of overlearning if data is limited (which is the case here. 5K sms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    # slow : 12 min for 100 epochs\n",
    "    # tf.keras.layers.Embedding(tokenizer.num_words + 1, 64, input_shape=[sms.shape[1],], name=\"embedding\"),\n",
    "    # tf.keras.layers.LSTM(units=64, return_sequences=True, name=\"lstm_1\"),  \n",
    "    # tf.keras.layers.LSTM(units=32, return_sequences=False, name=\"lstm_2\"), \n",
    "    # tf.keras.layers.Dense(16, activation='relu', name=\"dense_1\"),\n",
    "    # tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"dense_2\")\n",
    "\n",
    "    tf.keras.layers.Embedding(tokenizer.num_words + 1, 16, input_shape=[sequence_length], name=\"embedding\"),\n",
    "    tf.keras.layers.LSTM(units=16, return_sequences=True, name=\"lstm_1\"),  \n",
    "    tf.keras.layers.LSTM(units=8, return_sequences=False, name=\"lstm_2\"), \n",
    "    tf.keras.layers.Dense(8, activation='relu', name=\"dense_1\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"dense_2\")\n",
    "\n",
    "    # ! Never tried see https://medium.com/towards-data-science/spam-detection-in-emails-de0398ea3b48\n",
    "    # Creating an embedding layer to vectorize\n",
    "    # model.add(Embedding(max_feature, embedding_vector_length, input_length=max_len))\n",
    "    # Addding Bi-directional LSTM\n",
    "    # model.add(Bidirectional(tf.keras.layers.LSTM(64)))\n",
    "    # Relu allows converging quickly and allows backpropagation\n",
    "    # model.add(Dense(16, activation='relu'))\n",
    "    # Deep Learninng models can be overfit easily, to avoid this, we add randomization using drop out\n",
    "    # model.add(Dropout(0.1))\n",
    "    # Adding sigmoid activation function to normalize the output\n",
    "    # model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(f\"{k_Current_dir/k_AssetsDir/'lstm_arch.png'}\")\n",
    "tf.keras.utils.plot_model(model, path, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.Recall(name=\"recall\"), tf.keras.metrics.Precision(name=\"precision\"), \"accuracy\"]       # name=... avoid recall_1 for example\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_batch,\n",
    "    epochs = k_epochs,\n",
    "    validation_data=test_batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], color=\"b\", label=\"Train Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], color=\"r\", label=\"Val Loss\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"LSTM : Loss\")\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "save_fig(\"lstm_loss\", \"png\")\n",
    "plt.show()\n",
    "\n",
    "# display(history.history['loss'][-10:])\n",
    "display([[round(f, 6) for f in history.history['loss'][-10:]]])\n",
    "display([[round(f, 6) for f in history.history['val_loss'][-10:]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"accuracy\"], color=\"b\", label=\"Train Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], color=\"r\", label=\"Val Accuracy\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"LSTM : Accuracy\")\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "save_fig(\"lstm_accuracy\", \"png\")\n",
    "plt.show()\n",
    "\n",
    "display([[round(f, 6) for f in history.history['accuracy'][-10:]]])\n",
    "display([[round(f, 6) for f in history.history['val_accuracy'][-10:]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "* Let's try another model...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a CNN model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CNNs can be used for text classification since they can capture local patterns (phrases or word combinations).\n",
    "* Advantages:\n",
    "    * Quick to train\n",
    "    * Effective for capturing local features\n",
    "* Disadvantages:\n",
    "    * Can be less effective at capturing long-distance relationships in text\n",
    "    * We should not have issue here since sms are short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    # tf.keras.layers.Embedding(input_dim=k_num_words, output_dim=embedding_dim, input_length=sms.shape[1]),\n",
    "    # tf.keras.layers.Embedding(tokenizer.num_words + 1, 8, input_shape=[sms.shape[1],], name=\"embedding\"),\n",
    "    \n",
    "    # Might be too complex\n",
    "    # tf.keras.layers.Embedding(tokenizer.num_words + 1, 128, input_shape=[sms.shape[1],], name=\"embedding\"),\n",
    "    # tf.keras.layers.Conv1D(filters=64, kernel_size=5, activation='relu'),\n",
    "    # tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    # tf.keras.layers.Dense(64, activation='relu'),\n",
    "    # tf.keras.layers.Dropout(0.5),\n",
    "    # tf.keras.layers.Dense(1, activation='sigmoid') \n",
    "\n",
    "    # tf.keras.layers.Embedding(tokenizer.num_words + 1, 16, input_shape=[sms.shape[1],], name=\"embedding\"),\n",
    "    tf.keras.layers.Embedding(tokenizer.num_words + 1, 16, input_shape=[sequence_length], name=\"embedding\"),\n",
    "    tf.keras.layers.Conv1D(filters=8, kernel_size=5, activation='relu'),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(f\"{k_Current_dir/k_AssetsDir/'cnn_arch.png'}\")\n",
    "tf.keras.utils.plot_model(model, path, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss=tf.keras.losses.BinaryCrossentropy(), \n",
    "    # metrics=['accuracy']),\n",
    "    metrics=[tf.keras.metrics.Recall(name=\"recall\"), tf.keras.metrics.Precision(name=\"precision\"), \"accuracy\"]       # name=... avoid recall_1 for example\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_batch,\n",
    "    epochs = k_epochs,\n",
    "    validation_data=test_batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], color=\"b\", label=\"Train Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], color=\"r\", label=\"Val Loss\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"CNN : Loss\")\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "save_fig(\"cnn_loss\")\n",
    "plt.show()\n",
    "\n",
    "# display(history.history['loss'][-10:])\n",
    "display([[round(f, 6) for f in history.history['loss'][-10:]]])\n",
    "display([[round(f, 6) for f in history.history['val_loss'][-10:]]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "* Note how to model has been simplyfied\n",
    "* Also note the a dropout layer (randomly remove some features) help to fight overfitting\n",
    "* We could try to apply L1 or L2 regularization (add a cost to the loss function) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"accuracy\"], color=\"b\", label=\"Train Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], color=\"r\", label=\"Val Accuracy\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"CNN : Accuracy\")\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "save_fig(\"cnn_accuracy\")\n",
    "plt.show()\n",
    "\n",
    "display([[round(f, 6) for f in history.history['accuracy'][-10:]]])\n",
    "display([[round(f, 6) for f in history.history['val_accuracy'][-10:]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"precision\"], color=\"b\", label=\"Train Precision\")\n",
    "plt.plot(history.history[\"val_precision\"], color=\"r\", label=\"Val Precision\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"CNN : Precision\")\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "save_fig(\"cnn_precision\")\n",
    "plt.show()\n",
    "\n",
    "display([[round(f, 6) for f in history.history['precision'][-10:]]])\n",
    "display([[round(f, 6) for f in history.history['val_precision'][-10:]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"recall\"], color=\"b\", label=\"Train Recall\")\n",
    "plt.plot(history.history[\"val_recall\"], color=\"r\", label=\"Val Recall\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"CNN : Recall\")\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "save_fig(\"cnn_recall\")\n",
    "plt.show()\n",
    "\n",
    "display([[round(f, 6) for f in history.history['recall'][-10:]]])\n",
    "display([[round(f, 6) for f in history.history['val_recall'][-10:]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = f1_calculus(\"f1\", history.history[\"recall\"], history.history[\"precision\"])\n",
    "df_val_tmp = f1_calculus(\"val_f1\", history.history[\"val_recall\"], history.history[\"val_precision\"])\n",
    "\n",
    "plt.plot(df_tmp[\"f1\"], color=\"b\", label=\"Train F1\")\n",
    "plt.plot(df_val_tmp[\"val_f1\"], color=\"r\", label=\"Val F1\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"CNN : F1\")\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "save_fig(\"cnn_f1\", \"png\")\n",
    "plt.show()\n",
    "\n",
    "display(df_tmp.tail(10))\n",
    "display(df_val_tmp.tail(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning : BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\"><b>Comments :</b></span>\n",
    "* See 03-bert.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><b>Scrap book - Please ignore</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_Current_dir/k_AssetsDir/fig_id\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = Path(f\"{k_Current_dir/k_AssetsDir/fig_id}.{ext}\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_id = \"bob\"\n",
    "ext = \"png\"\n",
    "filename = Path(f\"{k_Current_dir/k_AssetsDir/fig_id}.{ext}\")\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()  \n",
    "ax.set(\n",
    "    title=\"Confusion Matrix on Train set\"\n",
    ")  \n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    classifier, \n",
    "    X_train, \n",
    "    Y_train, \n",
    "    ax=ax,\n",
    ")  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/60860121/plotly-how-to-make-an-annotated-confusion-matrix-using-a-heatmap\n",
    "\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "z = [[0.1, 0.3, 0.5, 0.2],\n",
    "     [1.0, 0.8, 0.6, 0.1],\n",
    "     [0.1, 0.3, 0.6, 0.9],\n",
    "     [0.6, 0.4, 0.2, 0.2]]\n",
    "\n",
    "x = ['healthy', 'multiple diseases', 'rust', 'scab']\n",
    "y =  ['healthy', 'multiple diseases', 'rust', 'scab']\n",
    "\n",
    "# change each element of z to type string for annotations\n",
    "z_text = [[str(y) for y in x] for x in z]\n",
    "\n",
    "# set up figure \n",
    "fig = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=z_text, colorscale='Viridis')\n",
    "\n",
    "# add title\n",
    "fig.update_layout(title_text='<i><b>Confusion matrix</b></i>',\n",
    "                  #xaxis = dict(title='x'),\n",
    "                  #yaxis = dict(title='x')\n",
    "                 )\n",
    "\n",
    "# add custom xaxis title\n",
    "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                        x=0.5,\n",
    "                        y=-0.15,\n",
    "                        showarrow=False,\n",
    "                        text=\"Predicted value\",\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\"))\n",
    "\n",
    "# add custom yaxis title\n",
    "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                        x=-0.35,\n",
    "                        y=0.5,\n",
    "                        showarrow=False,\n",
    "                        text=\"Real value\",\n",
    "                        textangle=-90,\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\"))\n",
    "\n",
    "# adjust margins to make room for yaxis title\n",
    "fig.update_layout(margin=dict(t=50, l=200))\n",
    "\n",
    "# add colorbar\n",
    "fig['data'][0]['showscale'] = True\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentences = [\n",
    "    'Le chat mange une souris',\n",
    "    'Le chien aboie fort'\n",
    "]\n",
    "\n",
    "new_sentence = ['Le lion rugit']\n",
    "\n",
    "# --------------------------\n",
    "tokenizer_no_oov = Tokenizer(num_words=10)\n",
    "tokenizer_no_oov.fit_on_texts(sentences)\n",
    "print(\"Voc sans oov_token      :\", tokenizer_no_oov.word_index)\n",
    "\n",
    "sequence_no_oov = tokenizer_no_oov.texts_to_sequences(new_sentence)\n",
    "print(\"Séquence sans oov_token :\", sequence_no_oov)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "tokenizer_with_oov = Tokenizer(num_words=10, oov_token='<OOV>')\n",
    "tokenizer_with_oov.fit_on_texts(sentences)\n",
    "print(\"Voc avec oov_token      :\", tokenizer_with_oov.word_index)\n",
    "\n",
    "sequence_with_oov = tokenizer_with_oov.texts_to_sequences(new_sentence)\n",
    "print(\"Séquence avec oov_token :\", sequence_with_oov)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jedha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
