{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\phili\\anaconda3\\envs\\tf_cpu2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# This code use \n",
    "#   BertForSequenceClassification\n",
    "#   bert-base-uncased (where cat & CAT are the same)\n",
    "\n",
    "# prelude\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "# import numpy as np\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer # 'TF' for TensorFlow models\n",
    "# , trainer, TrainingArguments           \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pathlib import Path\n",
    "k_Current_dir = Path.cwd()\n",
    "k_AssetsDir = \"assets\"\n",
    "k_sms_max_len = 100\n",
    "k_random_state  = 42\n",
    "k_test_size     = 0.3\n",
    "\n",
    "\n",
    "# Regarding the warning : TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html from .autonotebook import tqdm as notebook_tqdm\n",
    "# Solution ? \n",
    "# ! NOT TESTED YET - Does it apply to Jupyter \"in\" VSCode ?\n",
    "# https://saturncloud.io/blog/importerror-iprogress-not-found-please-update-jupyter-and-ipywidgets-although-it-is-installed/\n",
    "# conda install -c conda-forge ipywidgets\n",
    "# jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# drop empty cols and duplicates, rename cols...\n",
    "def cleaner(df):\n",
    "    df.drop(columns=\"Unnamed: 2\", inplace=True)\n",
    "    df.drop(columns=\"Unnamed: 3\", inplace=True)\n",
    "    df.drop(columns=\"Unnamed: 4\", inplace=True)\n",
    "\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df.columns = df.columns.str.replace(\"/\", \"_\")\n",
    "\n",
    "    df.rename(columns={\"v1\": \"labels\"}, inplace=True)\n",
    "    df.rename(columns={\"v2\": \"texts\"}, inplace=True)\n",
    "\n",
    "    df[\"labels\"] = df[\"labels\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = Path(f\"{k_Current_dir/k_AssetsDir/fig_id}.{fig_extension}\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(k_Current_dir / k_AssetsDir / \"spam.csv\", encoding=\"cp1252\")\n",
    "# df = cleaner(df)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df['sms'], df['label'], test_size=k_test_size, random_state=k_random_state, stratify=df['label'])\n",
    "\n",
    "# model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # modèle TensorFlow\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # On ne peut pas utiliser la classe trainer de Pytorch\n",
    "# # Il faut utiliser les outils TensorFlow\n",
    "\n",
    "# early_stopping = EarlyStopping(\n",
    "#     monitor='val_loss',           # can be 'val_accuracy' if needed \n",
    "#     patience=3,          \n",
    "#     restore_best_weights=True  \n",
    "# )\n",
    "\n",
    "# reduce_lr = ReduceLROnPlateau(\n",
    "#     monitor='val_loss', \n",
    "#     factor=0.2,       # reduction factor of learning rate\n",
    "#     patience=2,       \n",
    "#     min_lr=1e-7       # minimal value for learning rate\n",
    "# )\n",
    "\n",
    "# path = Path(f\"{k_Current_dir/k_AssetsDir/'bert_base_uncased_best_model.h5'}\")\n",
    "# checkpoint = ModelCheckpoint(\n",
    "#     path,                       # model's path\n",
    "#     monitor='val_loss', \n",
    "#     save_best_only=True, \n",
    "#     mode='min'\n",
    "# )\n",
    "\n",
    "# tensorboard = TensorBoard(log_dir='logs', histogram_freq=1)\n",
    "# print(f\"\\n\\n--------------------------------------------------\")\n",
    "# print(f\"Once the model runs, open a terminal, make sure you are in the directory of the project and type in : \")\n",
    "# print(f\"tensorboard --logdir=logs\")\n",
    "# print(f\"Then visit the URL\")\n",
    "\n",
    "# model.compile(\n",
    "#     optimizer=Adam(learning_rate=3e-5), \n",
    "#     loss='binary_crossentropy', \n",
    "#     metrics=[tf.keras.metrics.Recall(name=\"recall\"), tf.keras.metrics.Precision(name=\"precision\"), \"accuracy\"],       # name=... avoid recall_1 for example\n",
    "# )\n",
    "\n",
    "# # Tokenisation des textes pour l'entrée du modèle\n",
    "# train_encodings = tokenizer(\n",
    "#     X_train.tolist(),\n",
    "#     truncation=True,           # tronque si le texte est trop long pour BERT\n",
    "#     padding=True,              # ajoute du padding pour aligner les longueurs des séquences\n",
    "#     max_length=128,            # longueur max des séquences\n",
    "#     return_tensors='tf'        # retourne des tenseurs TensorFlow\n",
    "# )\n",
    "\n",
    "# test_encodings = tokenizer(\n",
    "#     X_test.tolist(),\n",
    "#     truncation=True,\n",
    "#     padding=True,\n",
    "#     max_length=128,\n",
    "#     return_tensors='tf'\n",
    "# )\n",
    "\n",
    "# # Conversion des labels en tenseurs TensorFlow\n",
    "# y_train_tf = tf.convert_to_tensor(y_train.tolist())\n",
    "# y_test_tf = tf.convert_to_tensor(y_test.tolist())\n",
    "\n",
    "# # Entraînement du modèle\n",
    "# history = model.fit(\n",
    "#     x={\"input_ids\": train_encodings['input_ids'], \"attention_mask\": train_encodings['attention_mask']},   # entrée des tokens et des masques\n",
    "#     y=y_train_tf,                          # les labels\n",
    "#     validation_data=(\n",
    "#         {\"input_ids\": test_encodings['input_ids'], \"attention_mask\": test_encodings['attention_mask']},  # entrée de validation\n",
    "#         y_test_tf                           # labels de validation\n",
    "#     ),\n",
    "#     batch_size=32,\n",
    "#     epochs=50,\n",
    "#     callbacks=[early_stopping, reduce_lr, checkpoint, tensorboard]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0 (unused)\n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 0\n",
      "Non-trainable params: 109,483,778\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\phili\\anaconda3\\envs\\tf_cpu2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Once the model runs, open a terminal, make sure you are in the directory of the project and type in : \n",
      "tensorboard --logdir=logs\n",
      "Then visit the URL\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\phili\\anaconda3\\envs\\tf_cpu2\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\phili\\anaconda3\\envs\\tf_cpu2\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\phili\\anaconda3\\envs\\tf_cpu2\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\phili\\anaconda3\\envs\\tf_cpu2\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1630, in train_step\n        x, y, sample_weight = keras.utils.unpack_x_y_sample_weight(data)\n\n    AttributeError: module 'keras.utils' has no attribute 'unpack_x_y_sample_weight'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 126\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThen visit the URL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    120\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m    121\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-5\u001b[39m), \n\u001b[0;32m    122\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m    123\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mRecall(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m\"\u001b[39m), tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mPrecision(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m],       \u001b[38;5;66;03m# name=... avoid recall_1 for example\u001b[39;00m\n\u001b[0;32m    124\u001b[0m )\n\u001b[1;32m--> 126\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensorboard\u001b[49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\phili\\anaconda3\\envs\\tf_cpu2\\lib\\site-packages\\transformers\\modeling_tf_utils.py:1229\u001b[0m, in \u001b[0;36mTFPreTrainedModel.fit\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mfit)\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1228\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\phili\\anaconda3\\envs\\tf_cpu2\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filesnpb_jyb.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\phili\\anaconda3\\envs\\tf_cpu2\\lib\\site-packages\\transformers\\modeling_tf_utils.py:1630\u001b[0m, in \u001b[0;36mTFPreTrainedModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_using_dummy_loss \u001b[38;5;129;01mand\u001b[39;00m parse(tf\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m parse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.11.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1628\u001b[0m     \u001b[38;5;66;03m# Newer TF train steps leave this out\u001b[39;00m\n\u001b[0;32m   1629\u001b[0m     data \u001b[38;5;241m=\u001b[39m expand_1d(data)\n\u001b[1;32m-> 1630\u001b[0m x, y, sample_weight \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_x_y_sample_weight\u001b[49m(data)\n\u001b[0;32m   1631\u001b[0m \u001b[38;5;66;03m# If the inputs are mutable dictionaries, make a shallow copy of them because we will modify\u001b[39;00m\n\u001b[0;32m   1632\u001b[0m \u001b[38;5;66;03m# them during input/label pre-processing. This avoids surprising the user by wrecking their data.\u001b[39;00m\n\u001b[0;32m   1633\u001b[0m \u001b[38;5;66;03m# In addition, modifying mutable Python inputs makes XLA compilation impossible.\u001b[39;00m\n\u001b[0;32m   1634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[1;31mAttributeError\u001b[0m: in user code:\n\n    File \"c:\\Users\\phili\\anaconda3\\envs\\tf_cpu2\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\phili\\anaconda3\\envs\\tf_cpu2\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\phili\\anaconda3\\envs\\tf_cpu2\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\phili\\anaconda3\\envs\\tf_cpu2\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1630, in train_step\n        x, y, sample_weight = keras.utils.unpack_x_y_sample_weight(data)\n\n    AttributeError: module 'keras.utils' has no attribute 'unpack_x_y_sample_weight'\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(k_Current_dir / k_AssetsDir / \"spam.csv\", encoding=\"cp1252\")\n",
    "df = cleaner(df)\n",
    "labels = df['labels'].tolist()\n",
    "texts = df['texts'].tolist()\n",
    "\n",
    "# BERT - Bidirectional Encoder Representations from Transformers\n",
    "# It use multiple inputs (input_ids & attention_mask)\n",
    "# We cannot use model = tf.keras.Sequential([...])\n",
    "\n",
    "# define both inputs \n",
    "# input_ids = id of the token as defined in the pre-trainned model \n",
    "# Attention_masks are used to indicate which parts of the sequence should be taken into account by the model\n",
    "# \"Hello, how are you?\"\n",
    "# [7592, 1010, 2129, 2024, 2017, 1029]        input_ids\n",
    "# [7592, 1010, 2129, 2024, 2017, 1029, 0, 0]  input_ids with padding\n",
    "# [   1,    1,    1,    1,    1,    1, 0, 0]  attention_masks with padding\n",
    "input_ids       = Input(shape=(k_sms_max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_masks = Input(shape=(k_sms_max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "# Load TensorFlow pretrained model from Hugging Face \n",
    "# 12-layers, 768-hidden-nodes, 12-attention-heads, 110M parameters\n",
    "# bert-base-cased : cat & CAT are different\n",
    "# bert_model = TFBertModel.from_pretrained('bert-base-cased')\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # modèle TensorFlow\n",
    "# Freeze all trainable parameters from all the layers of BERT\n",
    "for layer in bert_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# ! ATTENTION\n",
    "# If we want to freeze all but NOT the last 2 layers\n",
    "# BERT basic is made up of 12 stacked layers of transformers \n",
    "# Each transformer layer is made up of sub-layers, including attention mechanisms and feed-forward neural networks.\n",
    "# So before to \"unfreeze\" the last layer, some research might be required in order to unfreeze the layers correctly\n",
    "# for layer in bert_model.encoder.layer[-2:]:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# embeddings = bert_model(input_ids, attention_mask=attention_masks)[0]\n",
    "\n",
    "# Get the CLS token from the embeddings\n",
    "# cls_token = embeddings[:, 0, :]\n",
    "\n",
    "# Add a \"custom\" dense layer with sigmoid activation to BERT\n",
    "# output = Dense(1, activation='sigmoid')(cls_token)\n",
    "\n",
    "# Define the model\n",
    "# model = Model(inputs=[input_ids, attention_masks], outputs=output)\n",
    "\n",
    "model = bert_model\n",
    "model.summary()\n",
    "\n",
    "path = Path(f\"{k_Current_dir/k_AssetsDir/'TFBertForSequenceClassification_arch.png'}\")\n",
    "tf.keras.utils.plot_model(model, path, show_shapes=True)\n",
    "\n",
    "# encode sms with BERT tokenizer \n",
    "# bert-base-cased : cat & CAT are different\n",
    "# DONE : make a test with bert-base-uncased then bert-base-cased and compare\n",
    "# uncased : the model does not take the case into account\n",
    "# cased   : the model takes the case into account\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    \n",
    "encoded_data = tokenizer(\n",
    "    texts,\n",
    "    max_length=k_sms_max_len,\n",
    "    padding='max_length',       # sequences will be padded according the value of the parameter max_length\n",
    "    truncation=True,\n",
    "    return_tensors='tf'         # \"tf\" for TensorFlow\n",
    ")\n",
    "\n",
    "X_train_ids, X_test_ids, X_train_mask, X_test_mask, y_train, y_test = train_test_split(\n",
    "    encoded_data['input_ids'].numpy(), \n",
    "    encoded_data['attention_mask'].numpy(), \n",
    "    labels, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# convert the datasets into tensors\n",
    "X_train_ids     = tf.convert_to_tensor(X_train_ids)\n",
    "X_test_ids      = tf.convert_to_tensor(X_test_ids)\n",
    "X_train_mask    = tf.convert_to_tensor(X_train_mask)\n",
    "X_test_mask     = tf.convert_to_tensor(X_test_mask)\n",
    "y_train         = tf.convert_to_tensor(y_train)\n",
    "y_test          = tf.convert_to_tensor(y_test)\n",
    "\n",
    "# Gather encoded data into dictionaries for training\n",
    "X_train = {'input_ids': X_train_ids, 'attention_mask': X_train_mask}\n",
    "X_test = {'input_ids': X_test_ids, 'attention_mask': X_test_mask}\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',           # can be 'val_accuracy' if needed \n",
    "    patience=3,          \n",
    "    restore_best_weights=True  \n",
    ")\n",
    "\n",
    "# Reduces the learning rate when it stops improving\n",
    "# helps to converge more quickly to a minimum\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.2,       # reduction factor of learning rate\n",
    "    patience=2,       \n",
    "    min_lr=1e-7       # minimal value for learning rate\n",
    ")\n",
    "\n",
    "path = Path(f\"{k_Current_dir/k_AssetsDir/'bert_base_cased_best_model.h5'}\")\n",
    "checkpoint = ModelCheckpoint(\n",
    "    path,                       # model's path\n",
    "    monitor='val_loss', \n",
    "    save_best_only=True, \n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='logs', histogram_freq=1)\n",
    "print(f\"\\n\\n--------------------------------------------------\")\n",
    "print(f\"Once the model runs, open a terminal, make sure you are in the directory of the project and type in : \")\n",
    "print(f\"tensorboard --logdir=logs\")\n",
    "print(f\"Then visit the URL\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=3e-5), \n",
    "    loss='binary_crossentropy', \n",
    "    metrics=[tf.keras.metrics.Recall(name=\"recall\"), tf.keras.metrics.Precision(name=\"precision\"), \"accuracy\"],       # name=... avoid recall_1 for example\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train['input_ids'], X_train['attention_mask']],\n",
    "    y_train,\n",
    "    validation_data=([X_test['input_ids'], X_test['attention_mask']], y_test),\n",
    "    batch_size = 32,\n",
    "    epochs = 50,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint, tensorboard]  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(f\"{k_Current_dir/k_AssetsDir/'bert_base_uncased_arch.png'}\")\n",
    "tf.keras.utils.plot_model(model, path, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], color=\"b\", label=\"Train Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], color=\"r\", label=\"Val Loss\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Bert Base Uncased : Loss\")\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "save_fig(\"bert_base_uncased_loss\", \"png\")\n",
    "plt.show()\n",
    "\n",
    "display([[round(f, 6) for f in history.history['loss'][-10:]]])\n",
    "display([[round(f, 6) for f in history.history['val_loss'][-10:]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"accuracy\"], color=\"b\", label=\"Train Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], color=\"r\", label=\"Val Accuracy\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Bert Base Uncased : Accuracy\")\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "save_fig(\"bert_base_uncased_accuracy\", \"png\")\n",
    "plt.show()\n",
    "\n",
    "display([[round(f, 6) for f in history.history['accuracy'][-10:]]])\n",
    "display([[round(f, 6) for f in history.history['val_accuracy'][-10:]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"precision\"], color=\"b\", label=\"Train Precision\")\n",
    "plt.plot(history.history[\"val_precision\"], color=\"r\", label=\"Val Precision\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Bert Base Uncased : Precision\")\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "save_fig(\"bert_base_uncased_precision\", \"png\")\n",
    "plt.show()\n",
    "\n",
    "display([[round(f, 6) for f in history.history['precision'][-10:]]])\n",
    "display([[round(f, 6) for f in history.history['val_precision'][-10:]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"recall\"], color=\"b\", label=\"Train Recall\")\n",
    "plt.plot(history.history[\"val_recall\"], color=\"r\", label=\"Val Recall\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Bert Base Uncased : Recall\")\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "save_fig(\"bert_base_uncased_recall\", \"png\")\n",
    "plt.show()\n",
    "\n",
    "display([[round(f, 6) for f in history.history['recall'][-10:]]])\n",
    "display([[round(f, 6) for f in history.history['val_recall'][-10:]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_calculus(name, rec, prec):\n",
    "    df_tmp=pd.DataFrame()\n",
    "    df_tmp[name] = 2*np.array(rec)*np.array(prec)/(np.array(rec)+np.array(prec)+tf.keras.backend.epsilon()) # epsilon avoid runtimeWarning: divide by zero encountered in divide...\n",
    "    return df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = f1_calculus(\"f1\", history.history[\"recall\"], history.history[\"precision\"])\n",
    "df_val_tmp = f1_calculus(\"val_f1\", history.history[\"val_recall\"], history.history[\"val_precision\"])\n",
    "\n",
    "plt.plot(df_tmp[\"f1\"], color=\"b\", label=\"Train F1\")\n",
    "plt.plot(df_val_tmp[\"val_f1\"], color=\"r\", label=\"Val F1\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Bert Base Uncased : F1\")\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "save_fig(\"bert_base_uncased_f1\", \"png\")\n",
    "plt.show()\n",
    "\n",
    "display(df_tmp.tail(10))\n",
    "display(df_val_tmp.tail(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "att",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
