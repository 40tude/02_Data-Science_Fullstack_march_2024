{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(k_Current_dir / k_AssetsDir / \"spam.csv\", encoding=\"cp1252\")\n",
    "df = cleaner(df)\n",
    "\n",
    "labels = df['labels'].tolist()\n",
    "texts = df['texts'].tolist()\n",
    "\n",
    "# BERT - Bidirectional Encoder Representations from Transformers\n",
    "# It use multiple inputs (input_ids & attention_mask)\n",
    "# We cannot use model = tf.keras.Sequential([...])\n",
    "\n",
    "# define both inputs \n",
    "# input_ids = id of the token as defined in the pre-trainned model \n",
    "# Attention_masks are used to indicate which parts of the sequence should be taken into account by the model\n",
    "# \"Hello, how are you?\"\n",
    "# [7592, 1010, 2129, 2024, 2017, 1029]        input_ids\n",
    "# [7592, 1010, 2129, 2024, 2017, 1029, 0, 0]  input_ids with padding\n",
    "# [   1,    1,    1,    1,    1,    1, 0, 0]  attention_masks with padding\n",
    "# input_ids       = Input(shape=(k_sms_max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "# attention_masks = Input(shape=(k_sms_max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "# Affichage des formes des données\n",
    "# print(f\"Forme des input_ids : {encoded_data['input_ids'].shape}\")\n",
    "# print(f\"Forme des attention_mask : {encoded_data['attention_mask'].shape}\")\n",
    "\n",
    "# Load TensorFlow pretrained model from Hugging Face \n",
    "# 12-layers, 768-hidden-nodes, 12-attention-heads, 110M parameters\n",
    "# bert-base-uncased : cat & CAT are the same\n",
    "# bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # modèle TensorFlow\n",
    "# Freeze all trainable parameters from all the layers of BERT model\n",
    "# for layer in bert_model.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# ! ATTENTION\n",
    "# If we want to freeze all but NOT the last 2 layers\n",
    "# BERT basic is made up of 12 stacked layers of transformers \n",
    "# Each transformer layer is made up of sub-layers, including attention mechanisms and feed-forward neural networks.\n",
    "# So before to \"unfreeze\" the last layer, some research might be required in order to unfreeze the layers correclty\n",
    "# for layer in bert_model.encoder.layer[-2:]:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# embeddings = bert_model(input_ids, attention_mask=attention_masks)[0]\n",
    "\n",
    "# Get the CLS token from the embeddings\n",
    "# cls_token = embeddings[:, 0, :]\n",
    "\n",
    "# Add a \"custom\" dense layer with sigmoid activation to BERT\n",
    "# output = Dense(1, activation='sigmoid')(cls_token)\n",
    "\n",
    "# Define the model\n",
    "# model = Model(inputs=[input_ids, attention_masks], outputs=output)\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# path = Path(f\"{k_Current_dir/k_AssetsDir/'bert_base_uncased_arch.png'}\")\n",
    "# tf.keras.utils.plot_model(model, path, show_shapes=True)\n",
    "\n",
    "# encode sms with BERT tokenizer \n",
    "# DONE : make a test with bert-base-uncased then bert-base-cased and compare\n",
    "# uncased : the model does not take the case into account \n",
    "# cased   : the model takes the case into account\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "texts_train, texts_eval, labels_train, labels_eval = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "encoded_data = tokenizer(\n",
    "    texts,\n",
    "    max_length=k_sms_max_len,\n",
    "    padding='max_length',       # sequences will be padded according the value of the parameter max_length\n",
    "    truncation=True,\n",
    "    return_tensors='tf'         # \"tf\" for TensorFlow\n",
    ")\n",
    "\n",
    "\n",
    "X_train_ids, X_test_ids, X_train_mask, X_test_mask, y_train, y_test = train_test_split(\n",
    "    encoded_data['input_ids'].numpy(), \n",
    "    encoded_data['attention_mask'].numpy(), \n",
    "    labels, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# convert the datasets into tensors\n",
    "X_train_ids     = tf.convert_to_tensor(X_train_ids)\n",
    "X_test_ids      = tf.convert_to_tensor(X_test_ids)\n",
    "X_train_mask    = tf.convert_to_tensor(X_train_mask)\n",
    "X_test_mask     = tf.convert_to_tensor(X_test_mask)\n",
    "y_train         = tf.convert_to_tensor(y_train)\n",
    "y_test          = tf.convert_to_tensor(y_test)\n",
    "\n",
    "# Gather encoded data into dictionaries for training\n",
    "X_train = {'input_ids': X_train_ids, 'attention_mask': X_train_mask}\n",
    "X_test = {'input_ids': X_test_ids, 'attention_mask': X_test_mask}\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',           # can be 'val_accuracy' if needed \n",
    "    patience=3,          \n",
    "    restore_best_weights=True  \n",
    ")\n",
    "\n",
    "# Reduces the learning rate when it stops improving\n",
    "# helps to converge more quickly to a minimum\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.2,       # reduction factor of learning rate\n",
    "    patience=2,       \n",
    "    min_lr=1e-7       # minimal value for learning rate\n",
    ")\n",
    "\n",
    "path = Path(f\"{k_Current_dir/k_AssetsDir/'bert_base_uncased_best_model.h5'}\")\n",
    "checkpoint = ModelCheckpoint(\n",
    "    path,                       # model's path\n",
    "    monitor='val_loss', \n",
    "    save_best_only=True, \n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='logs', histogram_freq=1)\n",
    "print(f\"\\n\\n--------------------------------------------------\")\n",
    "print(f\"Once the model runs, open a terminal, make sure you are in the directory of the project and type in : \")\n",
    "print(f\"tensorboard --logdir=logs\")\n",
    "print(f\"Then visit the URL\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=3e-5), \n",
    "    loss='binary_crossentropy', \n",
    "    metrics=[tf.keras.metrics.Recall(name=\"recall\"), tf.keras.metrics.Precision(name=\"precision\"), \"accuracy\"],       # name=... avoid recall_1 for example\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train['input_ids'], X_train['attention_mask']],\n",
    "    y_train,\n",
    "    validation_data=([X_test['input_ids'], X_test['attention_mask']], y_test),\n",
    "    batch_size = 32,\n",
    "    epochs = 50,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint, tensorboard]  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # ! 210 minutes...\n",
    "\n",
    "# # -----------------------------------------------------------------------------\n",
    "# df = pd.read_csv(k_Current_dir / k_AssetsDir / \"spam.csv\", encoding=\"cp1252\")\n",
    "# df = cleaner(df)\n",
    "# labels = df['labels'].tolist()\n",
    "# texts = df['texts'].tolist()\n",
    "\n",
    "# # BERT - Bidirectional Encoder Representations from Transformers\n",
    "# # It use multiple inputs (input_ids & attention_mask)\n",
    "# # We cannot use model = tf.keras.Sequential([...])\n",
    "\n",
    "# # define both inputs \n",
    "# # input_ids = id of the token as defined in the pre-trainned model \n",
    "# # Attention_masks are used to indicate which parts of the sequence should be taken into account by the model\n",
    "# # \"Hello, how are you?\"\n",
    "# # [7592, 1010, 2129, 2024, 2017, 1029]        input_ids\n",
    "# # [7592, 1010, 2129, 2024, 2017, 1029, 0, 0]  input_ids with padding\n",
    "# # [   1,    1,    1,    1,    1,    1, 0, 0]  attention_masks with padding\n",
    "# input_ids       = Input(shape=(k_sms_max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "# attention_masks = Input(shape=(k_sms_max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "# # Affichage des formes des données\n",
    "# # print(f\"Forme des input_ids : {encoded_data['input_ids'].shape}\")\n",
    "# # print(f\"Forme des attention_mask : {encoded_data['attention_mask'].shape}\")\n",
    "\n",
    "# # Load TensorFlow pretrained model from Hugging Face \n",
    "# # 12-layers, 768-hidden-nodes, 12-attention-heads, 110M parameters\n",
    "# # bert-base-uncased : cat & CAT are the same\n",
    "# bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "# # Freeze all trainable parameters from all the layers of BERT model\n",
    "# for layer in bert_model.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# # ! ATTENTION\n",
    "# # If we want to freeze all but NOT the last 2 layers\n",
    "# # BERT basic is made up of 12 stacked layers of transformers \n",
    "# # Each transformer layer is made up of sub-layers, including attention mechanisms and feed-forward neural networks.\n",
    "# # So before to \"unfreeze\" the last layer, some research might be required in order to unfreeze the layers correclty\n",
    "# # for layer in bert_model.encoder.layer[-2:]:\n",
    "# #     layer.trainable = False\n",
    "\n",
    "# embeddings = bert_model(input_ids, attention_mask=attention_masks)[0]\n",
    "\n",
    "# # Get the CLS token from the embeddings\n",
    "# cls_token = embeddings[:, 0, :]\n",
    "\n",
    "# # Add a \"custom\" dense layer with sigmoid activation to BERT\n",
    "# output = Dense(1, activation='sigmoid')(cls_token)\n",
    "\n",
    "# # Define the model\n",
    "# model = Model(inputs=[input_ids, attention_masks], outputs=output)\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# path = Path(f\"{k_Current_dir/k_AssetsDir/'bert_base_uncased_arch.png'}\")\n",
    "# tf.keras.utils.plot_model(model, path, show_shapes=True)\n",
    "\n",
    "# # encode sms with BERT tokenizer \n",
    "# # DONE : make a test with bert-base-uncased then bert-base-cased and compare\n",
    "# # uncased : the model does not take the case into account \n",
    "# # cased   : the model takes the case into account\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    \n",
    "# encoded_data = tokenizer(\n",
    "#     texts,\n",
    "#     max_length=k_sms_max_len,\n",
    "#     padding='max_length',       # sequences will be padded according the value of the parameter max_length\n",
    "#     truncation=True,\n",
    "#     return_tensors='tf'         # \"tf\" for TensorFlow\n",
    "# )\n",
    "\n",
    "\n",
    "# X_train_ids, X_test_ids, X_train_mask, X_test_mask, y_train, y_test = train_test_split(\n",
    "#     encoded_data['input_ids'].numpy(), \n",
    "#     encoded_data['attention_mask'].numpy(), \n",
    "#     labels, \n",
    "#     test_size=0.2, \n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # convert the datasets into tensors\n",
    "# X_train_ids     = tf.convert_to_tensor(X_train_ids)\n",
    "# X_test_ids      = tf.convert_to_tensor(X_test_ids)\n",
    "# X_train_mask    = tf.convert_to_tensor(X_train_mask)\n",
    "# X_test_mask     = tf.convert_to_tensor(X_test_mask)\n",
    "# y_train         = tf.convert_to_tensor(y_train)\n",
    "# y_test          = tf.convert_to_tensor(y_test)\n",
    "\n",
    "# # Gather encoded data into dictionaries for training\n",
    "# X_train = {'input_ids': X_train_ids, 'attention_mask': X_train_mask}\n",
    "# X_test = {'input_ids': X_test_ids, 'attention_mask': X_test_mask}\n",
    "\n",
    "# early_stopping = EarlyStopping(\n",
    "#     monitor='val_loss',           # can be 'val_accuracy' if needed \n",
    "#     patience=3,          \n",
    "#     restore_best_weights=True  \n",
    "# )\n",
    "\n",
    "# # Reduces the learning rate when it stops improving\n",
    "# # helps to converge more quickly to a minimum\n",
    "# reduce_lr = ReduceLROnPlateau(\n",
    "#     monitor='val_loss', \n",
    "#     factor=0.2,       # reduction factor of learning rate\n",
    "#     patience=2,       \n",
    "#     min_lr=1e-7       # minimal value for learning rate\n",
    "# )\n",
    "\n",
    "# path = Path(f\"{k_Current_dir/k_AssetsDir/'bert_base_uncased_best_model.h5'}\")\n",
    "# checkpoint = ModelCheckpoint(\n",
    "#     path,                       # model's path\n",
    "#     monitor='val_loss', \n",
    "#     save_best_only=True, \n",
    "#     mode='min'\n",
    "# )\n",
    "\n",
    "# tensorboard = TensorBoard(log_dir='logs', histogram_freq=1)\n",
    "# print(f\"\\n\\n--------------------------------------------------\")\n",
    "# print(f\"Once the model runs, open a terminal, make sure you are in the directory of the project and type in : \")\n",
    "# print(f\"tensorboard --logdir=logs\")\n",
    "# print(f\"Then visit the URL\")\n",
    "\n",
    "# model.compile(\n",
    "#     optimizer=Adam(learning_rate=3e-5), \n",
    "#     loss='binary_crossentropy', \n",
    "#     metrics=[tf.keras.metrics.Recall(name=\"recall\"), tf.keras.metrics.Precision(name=\"precision\"), \"accuracy\"],       # name=... avoid recall_1 for example\n",
    "# )\n",
    "\n",
    "# history = model.fit(\n",
    "#     [X_train['input_ids'], X_train['attention_mask']],\n",
    "#     y_train,\n",
    "#     validation_data=([X_test['input_ids'], X_test['attention_mask']], y_test),\n",
    "#     batch_size = 32,\n",
    "#     epochs = 50,\n",
    "#     callbacks=[early_stopping, reduce_lr, checkpoint, tensorboard]  \n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
