{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AT&T Spam Detector - BertForSequenceClassification - bert-base-uncased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\"><b>DOES'NT WORK</b></span>\n",
    "\n",
    "This code use \n",
    "* BertForSequenceClassification\n",
    "* bert-base-uncased (where cat & CAT are the same)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prelude\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer # 'TF' for TensorFlow models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "k_Current_dir = Path.cwd()\n",
    "k_AssetsDir = \"assets\"\n",
    "k_sms_max_len = 100\n",
    "k_random_state  = 42\n",
    "k_test_size     = 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# drop empty cols and duplicates, rename cols...\n",
    "def cleaner(df):\n",
    "    df.drop(columns=\"Unnamed: 2\", inplace=True)\n",
    "    df.drop(columns=\"Unnamed: 3\", inplace=True)\n",
    "    df.drop(columns=\"Unnamed: 4\", inplace=True)\n",
    "\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df.columns = df.columns.str.replace(\"/\", \"_\")\n",
    "\n",
    "    df.rename(columns={\"v1\": \"labels\"}, inplace=True)\n",
    "    df.rename(columns={\"v2\": \"texts\"}, inplace=True)\n",
    "\n",
    "    df[\"labels\"] = df[\"labels\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(k_Current_dir / k_AssetsDir / \"spam.csv\", encoding=\"cp1252\")\n",
    "df = cleaner(df)\n",
    "labels = df['labels'].tolist()\n",
    "texts = df['texts'].tolist()\n",
    "\n",
    "input_ids       = Input(shape=(k_sms_max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_masks = Input(shape=(k_sms_max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # modèle TensorFlow\n",
    "# ! Fige toutes les couches, y compris la tête de classification, ce qui rend tout le modèle non entraînable.\n",
    "# Pas une bonne idée\n",
    "# for layer in model.layers:\n",
    "\n",
    "# Ne fige que les couches internes de BERT. Permet d'entraîner la tête de classification\n",
    "# Plus malin\n",
    "for layer in model.bert.encoder.layer:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.summary()\n",
    "\n",
    "path = Path(f\"{k_Current_dir/k_AssetsDir/'TFBertForSequenceClassification_arch.png'}\")\n",
    "tf.keras.utils.plot_model(model, path, show_shapes=True)\n",
    "\n",
    "# bert-base-uncased : cat & CAT are equal\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "encoded_data = tokenizer(\n",
    "    texts,\n",
    "    max_length=k_sms_max_len,\n",
    "    padding='max_length',       # sequences will be padded according the value of the parameter max_length\n",
    "    truncation=True,\n",
    "    return_tensors='tf'         # \"tf\" for TensorFlow\n",
    ")\n",
    "\n",
    "X_train_ids, X_test_ids, X_train_mask, X_test_mask, y_train, y_test = train_test_split(\n",
    "    encoded_data['input_ids'].numpy(), \n",
    "    encoded_data['attention_mask'].numpy(), \n",
    "    labels, \n",
    "    test_size=k_test_size, \n",
    "    random_state=k_random_state\n",
    ")\n",
    "\n",
    "# convert the datasets into tensors\n",
    "X_train_ids     = tf.convert_to_tensor(X_train_ids)\n",
    "X_test_ids      = tf.convert_to_tensor(X_test_ids)\n",
    "X_train_mask    = tf.convert_to_tensor(X_train_mask)\n",
    "X_test_mask     = tf.convert_to_tensor(X_test_mask)\n",
    "y_train         = tf.convert_to_tensor(y_train)\n",
    "y_test          = tf.convert_to_tensor(y_test)\n",
    "\n",
    "# Gather encoded data into dictionaries for training\n",
    "X_train = {'input_ids': X_train_ids, 'attention_mask': X_train_mask}\n",
    "X_test = {'input_ids': X_test_ids, 'attention_mask': X_test_mask}\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',           # can be 'val_accuracy' if needed \n",
    "    patience=3,          \n",
    "    restore_best_weights=True  \n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.2,       # reduction factor of learning rate\n",
    "    patience=2,       \n",
    "    min_lr=1e-7       # minimal value for learning rate\n",
    ")\n",
    "\n",
    "path = Path(f\"{k_Current_dir/k_AssetsDir/'bert_base_cased_best_model.h5'}\")\n",
    "checkpoint = ModelCheckpoint(\n",
    "    path,                       # model's path\n",
    "    monitor='val_loss', \n",
    "    save_best_only=True, \n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='logs', histogram_freq=1)\n",
    "print(f\"\\n\\n--------------------------------------------------\")\n",
    "print(f\"Once the model runs, open a terminal, make sure you are in the directory of the project and type in : \")\n",
    "print(f\"tensorboard --logdir=logs\")\n",
    "print(f\"Then visit the URL\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=3e-5), \n",
    "    loss='binary_crossentropy', \n",
    "    metrics=[tf.keras.metrics.Recall(name=\"recall\"), tf.keras.metrics.Precision(name=\"precision\"), \"accuracy\"],       # name=... avoid recall_1 for example\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train['input_ids'], X_train['attention_mask']],\n",
    "    y_train,\n",
    "    validation_data=([X_test['input_ids'], X_test['attention_mask']], y_test),\n",
    "    batch_size = 32,\n",
    "    epochs = 50,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint, tensorboard]  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(f\"{k_Current_dir/k_AssetsDir/'bert_base_uncased_arch.png'}\")\n",
    "tf.keras.utils.plot_model(model, path, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = Path(f\"{k_Current_dir/k_AssetsDir/fig_id}.{fig_extension}\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], color=\"b\", label=\"Train Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], color=\"r\", label=\"Val Loss\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Bert Base Uncased : Loss\")\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "save_fig(\"bert_base_uncased_loss\", \"png\")\n",
    "plt.show()\n",
    "\n",
    "display([[round(f, 6) for f in history.history['loss'][-10:]]])\n",
    "display([[round(f, 6) for f in history.history['val_loss'][-10:]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"accuracy\"], color=\"b\", label=\"Train Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], color=\"r\", label=\"Val Accuracy\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Bert Base Uncased : Accuracy\")\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "save_fig(\"bert_base_uncased_accuracy\", \"png\")\n",
    "plt.show()\n",
    "\n",
    "display([[round(f, 6) for f in history.history['accuracy'][-10:]]])\n",
    "display([[round(f, 6) for f in history.history['val_accuracy'][-10:]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"precision\"], color=\"b\", label=\"Train Precision\")\n",
    "plt.plot(history.history[\"val_precision\"], color=\"r\", label=\"Val Precision\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Bert Base Uncased : Precision\")\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "save_fig(\"bert_base_uncased_precision\", \"png\")\n",
    "plt.show()\n",
    "\n",
    "display([[round(f, 6) for f in history.history['precision'][-10:]]])\n",
    "display([[round(f, 6) for f in history.history['val_precision'][-10:]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"recall\"], color=\"b\", label=\"Train Recall\")\n",
    "plt.plot(history.history[\"val_recall\"], color=\"r\", label=\"Val Recall\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Bert Base Uncased : Recall\")\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "save_fig(\"bert_base_uncased_recall\", \"png\")\n",
    "plt.show()\n",
    "\n",
    "display([[round(f, 6) for f in history.history['recall'][-10:]]])\n",
    "display([[round(f, 6) for f in history.history['val_recall'][-10:]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_calculus(name, rec, prec):\n",
    "    df_tmp=pd.DataFrame()\n",
    "    df_tmp[name] = 2*np.array(rec)*np.array(prec)/(np.array(rec)+np.array(prec)+tf.keras.backend.epsilon()) # epsilon avoid runtimeWarning: divide by zero encountered in divide...\n",
    "    return df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = f1_calculus(\"f1\", history.history[\"recall\"], history.history[\"precision\"])\n",
    "df_val_tmp = f1_calculus(\"val_f1\", history.history[\"val_recall\"], history.history[\"val_precision\"])\n",
    "\n",
    "plt.plot(df_tmp[\"f1\"], color=\"b\", label=\"Train F1\")\n",
    "plt.plot(df_val_tmp[\"val_f1\"], color=\"r\", label=\"Val F1\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Bert Base Uncased : F1\")\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "save_fig(\"bert_base_uncased_f1\", \"png\")\n",
    "plt.show()\n",
    "\n",
    "display(df_tmp.tail(10))\n",
    "display(df_val_tmp.tail(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "att",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
