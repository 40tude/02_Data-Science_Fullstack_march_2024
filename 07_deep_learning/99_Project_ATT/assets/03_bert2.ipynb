{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prelude\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pathlib import Path\n",
    "k_Current_dir = Path.cwd()\n",
    "k_AssetsDir = \"assets\"\n",
    "k_sms_max_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "def cleaner(df):\n",
    "    df.drop(columns=\"Unnamed: 2\", inplace=True)\n",
    "    df.drop(columns=\"Unnamed: 3\", inplace=True)\n",
    "    df.drop(columns=\"Unnamed: 4\", inplace=True)\n",
    "\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df.columns = df.columns.str.replace(\"/\", \"_\")\n",
    "\n",
    "    df.rename(columns={\"v1\": \"labels\"}, inplace=True)\n",
    "    df.rename(columns={\"v2\": \"texts\"}, inplace=True)\n",
    "\n",
    "    df[\"labels\"] = df[\"labels\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# max_len = max len of an sms\n",
    "def encode_texts(texts, tokenizer, max_len):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "df = pd.read_csv(k_Current_dir / k_AssetsDir / \"spam.csv\", encoding=\"cp1252\")\n",
    "df = cleaner(df)\n",
    "labels = df['labels'].tolist()\n",
    "texts = df['texts'].tolist()\n",
    "\n",
    "# Load the tokenizer and pre trained BERT model (here, basic uncased)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "input_ids = Input(shape=(k_sms_max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_masks = Input(shape=(k_sms_max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "# Feature extraction with BERT\n",
    "embeddings = bert_model(input_ids, attention_mask=attention_masks)[0]\n",
    "\n",
    "# Add a classification layer\n",
    "cls_token = embeddings[:, 0, :]\n",
    "output = Dense(1, activation='sigmoid')(cls_token)\n",
    "\n",
    "# Définir le modèle\n",
    "model = Model(inputs=[input_ids, attention_masks], outputs=output)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=3e-5), \n",
    "    loss='binary_crossentropy', \n",
    "    metrics=[tf.keras.metrics.Recall(name=\"recall\"), tf.keras.metrics.Precision(name=\"precision\"), \"accuracy\"],       # name=... avoid recall_1 for example\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "path = Path(f\"{k_Current_dir/k_AssetsDir/'bert_arch.png'}\")\n",
    "tf.keras.utils.plot_model(model, path, show_shapes=True)\n",
    "\n",
    "# encode sms with BERT tokenizer\n",
    "encoded_data = encode_texts(texts, tokenizer, k_sms_max_len)\n",
    "\n",
    "X_train_ids, X_test_ids, X_train_mask, X_test_mask, y_train, y_test = train_test_split(\n",
    "    encoded_data['input_ids'].numpy(), \n",
    "    encoded_data['attention_mask'].numpy(), \n",
    "    labels, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# convert dataset into tensors\n",
    "X_train_ids = tf.convert_to_tensor(X_train_ids)\n",
    "X_test_ids = tf.convert_to_tensor(X_test_ids)\n",
    "X_train_mask = tf.convert_to_tensor(X_train_mask)\n",
    "X_test_mask = tf.convert_to_tensor(X_test_mask)\n",
    "y_train = tf.convert_to_tensor(y_train)\n",
    "y_test = tf.convert_to_tensor(y_test)\n",
    "\n",
    "# Gather encoded data into dict for training\n",
    "X_train = {'input_ids': X_train_ids, 'attention_mask': X_train_mask}\n",
    "X_test = {'input_ids': X_test_ids, 'attention_mask': X_test_mask}\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # can be 'val_accuracy' if needed \n",
    "    patience=3,          \n",
    "    restore_best_weights=True  \n",
    ")\n",
    "\n",
    "# Reduces the learning rate when it stops improving\n",
    "# can help to converge more quickly to a minimum\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.2,       # reduction factor of learning rate\n",
    "    patience=2,       \n",
    "    min_lr=1e-7       # minimal value for learning rate\n",
    ")\n",
    "\n",
    "path = Path(f\"{k_Current_dir/k_AssetsDir/'best_model.h5'}\")\n",
    "checkpoint = ModelCheckpoint(\n",
    "    path,                       # model's path\n",
    "    monitor='val_loss', \n",
    "    save_best_only=True, \n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='logs', histogram_freq=1)\n",
    "print(f\"\\n\\n--------------------------------------------------\")\n",
    "print(f\"Once the model runs, open a terminal and type in : \")\n",
    "print(f\"tensorboard --logdir=logs\")\n",
    "print(f\"Then visit the URL\")\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train['input_ids'], X_train['attention_mask']],\n",
    "    y_train,\n",
    "    validation_data=([X_test['input_ids'], X_test['attention_mask']], y_test),\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint, tensorboard]  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], color=\"b\", label=\"Train Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], color=\"r\", label=\"Val Loss\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Basic : Loss\")\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "save_fig(\"basic_loss\", \"png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# display(history.history['loss'][-10:])\n",
    "display([[round(f, 6) for f in history.history['loss'][-10:]]])\n",
    "display([[round(f, 6) for f in history.history['val_loss'][-10:]]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "att",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
