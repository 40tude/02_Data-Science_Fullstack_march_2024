{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\phili\\anaconda3\\envs\\tf_cpu_bert_seq\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# conda create --name tf_cpu_bert_seq --file ./assets/requirements_tf_cpu_bert_seq_model.txt\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "import tensorflow as tf\n",
    "\n",
    "from pathlib import Path\n",
    "k_Current_dir = Path.cwd()\n",
    "k_AssetsDir = \"assets\"\n",
    "k_sms_max_len = 100\n",
    "k_random_state  = 42\n",
    "k_test_size     = 0.3\n",
    "k_batch_size = 32\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# drop empty cols and duplicates, rename cols...\n",
    "def cleaner(df):\n",
    "    df.drop(columns=\"Unnamed: 2\", inplace=True)\n",
    "    df.drop(columns=\"Unnamed: 3\", inplace=True)\n",
    "    df.drop(columns=\"Unnamed: 4\", inplace=True)\n",
    "\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df.columns = df.columns.str.replace(\"/\", \"_\")\n",
    "\n",
    "    df.rename(columns={\"v1\": \"labels\"}, inplace=True)\n",
    "    df.rename(columns={\"v2\": \"texts\"}, inplace=True)\n",
    "\n",
    "    df[\"labels\"] = df[\"labels\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_examples(texts, labels):\n",
    "    examples = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        examples.append(InputExample(guid=None, text_a=text, text_b=None, label=label))\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=k_sms_max_len):\n",
    "    features = []\n",
    "\n",
    "    for example in examples:\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            example.text_a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids, attention_mask = input_dict[\"input_ids\"], input_dict[\"attention_mask\"]\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                label=example.label\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def gen():\n",
    "        for feature in features:\n",
    "            yield (\n",
    "                {\n",
    "                    'input_ids': feature.input_ids,\n",
    "                    'attention_mask': feature.attention_mask\n",
    "                },\n",
    "                feature.label\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({'input_ids': tf.int32, 'attention_mask': tf.int32}, tf.int64),\n",
    "        ({'input_ids': tf.TensorShape([None]), 'attention_mask': tf.TensorShape([None])}, tf.TensorShape([])),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\phili\\anaconda3\\envs\\tf_cpu_bert_seq\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 24,429,314\n",
      "Non-trainable params: 85,054,464\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(k_Current_dir / k_AssetsDir / \"spam.csv\", encoding=\"cp1252\")\n",
    "df = cleaner(df)\n",
    "\n",
    "X = df['texts'].values\n",
    "y = df['labels'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=k_test_size, random_state=k_random_state)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_examples = convert_data_to_examples(X_train, y_train)\n",
    "test_examples  = convert_data_to_examples(X_test, y_test)\n",
    "\n",
    "train_dataset = convert_examples_to_tf_dataset(train_examples, tokenizer)\n",
    "test_dataset  = convert_examples_to_tf_dataset(test_examples, tokenizer)\n",
    "\n",
    "train_dataset = train_dataset.shuffle(100).batch(k_batch_size).repeat(2)\n",
    "test_dataset  = test_dataset.batch(k_batch_size)\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# ! Fige toutes les couches, y compris la tête de classification, ce qui rend tout le modèle non entraînable.\n",
    "# Pas une bonne idée\n",
    "# for layer in model.layers:\n",
    "\n",
    "# Ne fige que les couches internes de BERT. Permet d'entraîner la tête de classification\n",
    "# Plus malin\n",
    "for layer in model.bert.encoder.layer:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.summary()\n",
    "\n",
    "path = Path(f\"{k_Current_dir/k_AssetsDir/'bert_seq_uncased_01_arch.png'}\")\n",
    "tf.keras.utils.plot_model(model, path, show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "260/260 [==============================] - 1598s 6s/step - loss: 0.2126 - accuracy: 0.9253 - val_loss: 0.0627 - val_accuracy: 0.9836\n",
      "Epoch 2/2\n",
      "260/260 [==============================] - 2145s 8s/step - loss: 0.0408 - accuracy: 0.9891 - val_loss: 0.0392 - val_accuracy: 0.9845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x229be22c130>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "model.fit(\n",
    "    train_dataset, \n",
    "    epochs=2, \n",
    "    validation_data=test_dataset\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_cpu_bert_seq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
